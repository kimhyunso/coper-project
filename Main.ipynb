{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428056ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YoutubeModule Class\n",
    "from CrawlingModule import YoutubeBuilder\n",
    "import pandas as pd\n",
    "import Preprocessing as prep\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# KEY 파일로 읽어오기\n",
    "KEY_FILE = open('./API_KEY.txt', 'r')\n",
    "\n",
    "# API_KEY 할당\n",
    "API_KEY = KEY_FILE.readline().split(':')[1]\n",
    "\n",
    "# YoutbeModule Instarnce \n",
    "bulider = YoutubeBuilder(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channelId = bulider.search_get_channelId('할명수')\n",
    "# video_id_list = bulider.search_get_videoId_in_channel(channelId=channelId)\n",
    "\n",
    "# video_list = bulider.get_videos_in_videoId_list(videoId_list=video_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_list, none_video_id_list = bulider.get_comments(video_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3000bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(video_id_list, columns=['video_id'])\n",
    "# (df.video_id.value_counts() > 2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # video_id,category_id,category_name,title,views_count,like_count,uploaded_at\n",
    "# df = pd.DataFrame(video_list, columns=['video_id', 'category_id', 'category_name', 'title', 'views_count', 'like_count', 'uploaded_at', 'tags'])\n",
    "\n",
    "# df.tags = chim_df.tags.apply(lambda x : prep.extract_tags(x, name='#할명수 '))\n",
    "# df.uploaded_at = pd.to_datetime(chim_df.uploaded_at)\n",
    "# df.to_csv('./데이터/할명수_videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(comments_list, columns=['video_id', 'comment_id', 'comment', 'like_count', 'created_at', 'updated_at'])\n",
    "# df.created_at = pd.to_datetime(df.created_at)\n",
    "# df.updated_at = pd.to_datetime(df.updated_at)\n",
    "\n",
    "\n",
    "# df.to_csv('./데이터/할명수_comments.csv', index=False)\n",
    "# # df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de00f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chim_comment_df = pd.read_csv('./데이터/침착맨_commets.csv', encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chim_comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32c79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_video_id = chim_comment_df.video_id.unique()\n",
    "video_id_and_count = dict()\n",
    "\n",
    "for video_id in unique_video_id:\n",
    "    video_id_and_count[video_id] = chim_comment_df.loc[chim_comment_df.video_id == video_id].like_count.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de01678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9LQsLPsKRq8', 249220),\n",
       " ('53XqM_ofaKw', 190285),\n",
       " ('IfOSBmBCWw0', 179199),\n",
       " ('Z1vgOoIuP5M', 177677),\n",
       " ('pkXiOnXAO90', 159250),\n",
       " ('IxaOybVE50Y', 157263),\n",
       " ('hnanNlDbsE4', 156647),\n",
       " ('WeGDixN-u48', 138191),\n",
       " ('V1ryDPaC6mo', 124719),\n",
       " ('SP-LJqVgQuw', 120800)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비디오별 댓글 좋아요 수\n",
    "comments_like_sum = sorted(video_id_and_count.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "video_on_comments_like_sum = list()\n",
    "\n",
    "for idx, value in enumerate(comments_like_sum):\n",
    "    if idx == 10:\n",
    "        break\n",
    "    video_on_comments_like_sum.append(value)\n",
    "\n",
    "video_on_comments_like_sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf9340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:04<00:00,  6.42s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_list = []\n",
    "new_df = list()\n",
    "stopwords = prep.stopwords('./데이터/stopwords.txt')\n",
    "\n",
    "for video_id in tqdm(video_on_comments_like_sum):\n",
    "    new_df.append(chim_comment_df.loc[chim_comment_df.video_id == video_id[0]].comment.apply(lambda x : prep.text_cleaning(x, stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31810c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = r'C:/Windows/Fonts/malgun.ttf'\n",
    "youtube_mask = np.array(Image.open('./데이터/youtube_logo.jfif'))\n",
    "\n",
    "\n",
    "# def make_colors(word, font_size, position, orientation, random_state, **kwargs):\n",
    "#     r = random_state.randint(170, 220)\n",
    "#     g = random_state.randint(0, 20)\n",
    "#     b = random_state.randint(0, 60)\n",
    "    \n",
    "#     color = 'rgb(%d, %d, %d)' % (r,g,b)\n",
    "#     return color\n",
    "\n",
    "for df in new_df:\n",
    "    string_dict = dict()\n",
    "    for comment in df:\n",
    "        for key, value in Counter(comment).most_common(10):\n",
    "            string_dict[key] = value\n",
    "            \n",
    "    wc = WordCloud(font_path=font_path, background_color='white', stopwords=['개소리', '존나', '개', '진짜', '정신'], mask=youtube_mask,\n",
    "           max_font_size=30, scale=7).generate_from_frequencies(string_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # wc.recolor(color_func=make_colors, random_state=True)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.show()\n",
    "    del string_dict\n",
    "    # wc.to_file(filename=\"침착맨_원피스.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = r'C:/Windows/Fonts/malgun.ttf'\n",
    "youtube_mask = np.array(Image.open('./데이터/youtube_logo.jfif'))\n",
    "\n",
    "\n",
    "# def make_colors(word, font_size, position, orientation, random_state, **kwargs):\n",
    "#     r = random_state.randint(170, 220)\n",
    "#     g = random_state.randint(0, 20)\n",
    "#     b = random_state.randint(0, 60)\n",
    "    \n",
    "#     color = 'rgb(%d, %d, %d)' % (r,g,b)\n",
    "#     return color\n",
    "\n",
    "for df in new_df:\n",
    "    string = ''\n",
    "    for comment in df:\n",
    "        string += ' '.join(comment)\n",
    "        \n",
    "    wc = WordCloud(font_path=font_path, background_color='white', stopwords=['개소리', '존나', '개', '진짜'], mask=youtube_mask,\n",
    "           max_font_size=30, scale=7).generate(string)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # wc.recolor(color_func=make_colors, random_state=True)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.show()\n",
    "    del string\n",
    "    # wc.to_file(filename=\"침착맨_원피스.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"·\", \" \").strip()\n",
    "    pattern = '[^ ㄱ-ㅣ가-힣]+'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    return text\n",
    "\n",
    "def get_nouns(tokenizer, sentence):\n",
    "    tagged = tokenizer.pos(sentence)\n",
    "    nouns = [s for s, t in tagged if t in ['Noun', 'Verb'] and len(s) >1]\n",
    "    return nouns\n",
    "\n",
    "def tokenize(df):\n",
    "    tokenizer = Okt()\n",
    "    processed_data = []\n",
    "    for sent in tqdm(df['comment']):\n",
    "        sentence = clean_text(str(sent).replace(\"\\n\", \"\").strip())\n",
    "        processed_data.append(get_nouns(tokenizer, sentence))\n",
    "    return processed_data\n",
    "\n",
    "test = tokenize(chim_comment_df)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ad3eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 :  159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "strop_words = prep.stopwords('./데이터/stopwords.txt')\n",
    "\n",
    "# doc1 = vector.fit_transform(chim_comment_df[chim_comment_df.video_id == '9LQsLPsKRq8'].head(10).comment.apply(lambda x : prep.text_cleaning(x, strop_words)).iloc[7]).toarray()\n",
    "# doc2 = vector.fit_transform(chim_comment_df[chim_comment_df.video_id == '9LQsLPsKRq8'].head(10).comment.apply(lambda x : prep.text_cleaning(x, strop_words)).iloc[9]).toarray()\n",
    "# print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))\n",
    "# doc1, doc2\n",
    "\n",
    "max_len = max(len(item) for item in chim_comment_df[chim_comment_df.video_id == '9LQsLPsKRq8'].comment.apply(lambda x : prep.text_cleaning(x, strop_words)))\n",
    "\n",
    "print('최대 길이 : ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerizerm_comment_df.comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in chim_comment_df[chim_comment_df.video_id == '9LQsLPsKRq8'].comment.apply(lambda x : prep.text_cleaning(x, strop_words)).values:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b31a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = vector.fit_transform(a).toarray()\n",
    "doc2 = vector.fit_transform(b).toarray()\n",
    "print('문서 1과 문서2의 유사도 : \\n',cos_sim(doc1, doc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b39fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "dictionary = corpora.Dictionary(test)\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in test]\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe64ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lda_visualization = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(lda_visualization, 'file_name.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e46cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9366e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "coherence = cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dafd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67190083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a81b4bc3938bd8f3d69751270fa5baefbe4154c7526f8690ed0936808b926fab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
